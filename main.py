import os
import warnings

import faiss
import fitz
import gradio
import gradio as gr
from docx import Document
from langchain_community.vectorstores import Annoy
from langchain_text_splitters import RecursiveCharacterTextSplitter
from sentence_transformers import SentenceTransformer

from chain import RetrievalChain, RetrievalMode, SearchMode
from fileload import FileLoader

warnings.filterwarnings("ignore")

def load_document(file_path):
    ext = os.path.splitext(file_path)[1].lower()
    text = ""
    try:
        if ext == ".txt":
            with open(file_path, "r", encoding="utf-8") as f:
                text = f.read()
        elif ext == ".pdf":
            doc = fitz.open(file_path)
            for page in doc:
                text += page.get_text()
            doc.close()
        elif ext == ".docx":
            doc = Document(file_path)
            text = "\n".join([para.text for para in doc.paragraphs])
    except Exception as e:
        raise e
    return text

def split_text(text, chunk_size=256, overlap=32):
    """ç›¸é‚»å—ä¹‹é—´æœ‰32ä¸ªè¯çš„é‡å ï¼Œé¿å…å…³é”®ä¿¡æ¯è¢«åˆ†å‰²"""
    words = text.split()
    chunks = []
    start = 0
    while start < len(words):
        end = start + chunk_size
        chunk = " ".join(words[start:end])
        chunks.append(chunk)
        start = end - overlap
        if start >= len(words):
            break
    return chunks



def upload_file(files: list[gradio.utils.NamedString]):
    try:
        if not files:
            return "âŒ è¯·é€‰æ‹©è¦ä¸Šä¼ çš„æ–‡ä»¶"

        documents = []
        supported_exts = {".txt", ".pdf", ".docx"}
        processed_files = []

        # å¤„ç†ä¸Šä¼ çš„æ–‡ä»¶
        for uploaded_file in files:
            filename = os.path.basename(uploaded_file.name)
            file_ext = os.path.splitext(filename)[1].lower()
            
            # æ£€æŸ¥æ–‡ä»¶æ ¼å¼
            if file_ext not in supported_exts:
                continue
                
            try:
                # ç›´æ¥ä»ä¸Šä¼ çš„æ–‡ä»¶è·¯å¾„è¯»å–å†…å®¹
                text = load_document(uploaded_file.name)
                if text.strip():
                    chunks = split_text(text, chunk_size=256, overlap=32)
                    chunks_with_source = [f"[æ¥æº: {filename}]\n{chunk}" for chunk in chunks]
                    documents.extend(chunks_with_source)
                    processed_files.append(filename)
            except Exception as e:
                print(f"å¤„ç†æ–‡ä»¶ {filename} æ—¶å‡ºé”™: {e}")
                continue

        if not documents:
            return "âŒ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„æ–‡æ¡£å†…å®¹æˆ–æ–‡ä»¶æ ¼å¼ä¸æ”¯æŒï¼ˆä»…æ”¯æŒ .txt, .pdf, .docxï¼‰"

        # æ„å»ºå‘é‡æ•°æ®åº“
        print("å¼€å§‹æ„å»ºå‘é‡æ•°æ®åº“...")
        embedding_model_name = "all-MiniLM-L6-v2"
        embedding_model = SentenceTransformer(embedding_model_name)
        embeddings = embedding_model.encode(
            documents, convert_to_numpy=True, show_progress_bar=True
        )
        
        dimension = embeddings.shape[1]
        index = faiss.IndexFlatL2(dimension)
        index.add(embeddings)

        # æ›´æ–°æ£€ç´¢é“¾
        retrieval_chain.docs = documents
        retrieval_chain.emb_model = embedding_model
        retrieval_chain.index = index
        retrieval_chain.retriever_mode = RetrievalMode.WITH_FILE_RETRIEVER
        
        print("å‘é‡æ•°æ®åº“æ„å»ºå®Œæˆ!")
        return f"âœ… æˆåŠŸå¤„ç†äº† {len(processed_files)} ä¸ªæ–‡ä»¶ï¼Œæ„å»ºäº†åŒ…å« {len(documents)} ä¸ªæ–‡æ¡£å—çš„å‘é‡æ•°æ®åº“\nå¤„ç†çš„æ–‡ä»¶: {', '.join(processed_files)}"
        
    except Exception as e:
        error_msg = f"âŒ æ–‡ä»¶å¤„ç†å¤±è´¥: {str(e)}"
        print(error_msg)
        return error_msg

def submit_message(message, use_web):
    if use_web == "ä½¿ç”¨":
        retrieval_chain.search_mode = SearchMode.WEB
    retrieval_chain.chat_history.append(gr.ChatMessage(role="user", content=message))
    retrieval_chain.chat_history.append(gr.ChatMessage(role="assistant", content=retrieval_chain.invoke(message)))
    return "", retrieval_chain.chat_history

def clear_messages():
    return "", []

with gr.Blocks() as demo:
    gr.Markdown("""<h1><center>AI_LangChain</center></h1> <center><font size=3> </center></font>""")
    with gr.Row():
        with gr.Column(scale=1):
            # Radio å•é€‰æ¡†
            use_web = gr.Radio(["ä½¿ç”¨", "ä¸ä½¿ç”¨"], label="è”ç½‘æœç´¢", value="ä¸ä½¿ç”¨", interactive=True)
            # Fileä¸Šä¼ æ–‡ä»¶çš„ç»„ä»¶
            file = gr.File(label="ä¸Šä¼ æ–‡ä»¶ï¼Œä»…æ”¯æŒæ–‡æœ¬æ–‡ä»¶", file_count="multiple", file_types=['.txt', '.md', '.docx', '.pdf'])
            # çŠ¶æ€æ˜¾ç¤º
            status = gr.Textbox(label="å¤„ç†çŠ¶æ€", value="ç­‰å¾…ä¸Šä¼ æ–‡ä»¶...", interactive=False, lines=2)
        with gr.Column(scale=4):
            with gr.Row():
                chatbot = gr.Chatbot(type="messages", height=500)
            with gr.Row():
                message = gr.Textbox(label="è¾“å…¥é—®é¢˜", placeholder="è¯·è¾“å…¥é—®é¢˜", lines=1)
            with gr.Row():
                clear_history = gr.Button("ğŸ§¹ æ¸…é™¤å†å²å¯¹è¯")
                submit = gr.Button("ğŸš€ æäº¤")
        # ä¸Šä¼ æ–‡ä»¶åŠ¨ä½œ
        file.upload(fn=upload_file, inputs=file, outputs=status)
        # æäº¤æŒ‰é’®åŠ¨ä½œ
        submit.click(fn=submit_message, inputs=[message, use_web], outputs=[message, chatbot])
        # è¾“å…¥æ¡†å›è½¦
        message.submit(fn=submit_message, inputs=[message, use_web], outputs=[message, chatbot])
        # æ¸…ç©ºå†å²å¯¹è¯åŠ¨ä½œ
        clear_history.click(fn=clear_messages, inputs=[], outputs=[message, chatbot])



print("æ­£åœ¨åˆå§‹åŒ–æ£€ç´¢é“¾...")
try:
    retrieval_chain = RetrievalChain()
    print("æ£€ç´¢é“¾åˆå§‹åŒ–å®Œæˆ")
except Exception as e:
    print(f"æ£€ç´¢é“¾åˆå§‹åŒ–å¤±è´¥: {e}")
print("æ­£åœ¨å¯åŠ¨ Gradio ç•Œé¢...")
# è®°å¾—å…³VPN
demo.launch()